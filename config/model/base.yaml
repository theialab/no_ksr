# Managed by Hydra

ckpt_path: 

logger:
  # https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html
  _target_: pytorch_lightning.loggers.WandbLogger
  project: noksr
  name: ${experiment_name}

# https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
trainer:
  accelerator: gpu #cpu or gpu
  devices: auto
  num_nodes: 1
  max_epochs: 800
  max_steps: 200000
  num_sanity_val_steps: 8
  check_val_every_n_epoch: 4
  profiler: simple

# https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.ModelCheckpoint.html
checkpoint_monitor:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  save_top_k: -1
  every_n_epochs: 4
  filename: "{epoch}"
  dirpath: ${exp_output_root_path}/training


optimizer:
  name: Adam # SGD or Adam
  lr: 0.001
  warmup_steps_ratio: 0.1

lr_decay: # for Adam
  decay_start_epoch: 250


inference:
  split: test